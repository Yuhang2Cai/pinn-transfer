# train_tl.py
import os
import time

import numpy as np
import torch
from torch.utils.data import DataLoader
import pandas as pd
import matplotlib.pyplot as plt

from config import (
    DEVICE, BATCH_SIZE, NUM_EPOCH, NUM_LAYERS, NUM_NEURONS,
    LR, INPUT_DIM, OUTPUT_DIM, TIME_SCALE
)
from utils import (
    TensorDataset, standardize_tensor,
    calculate_metrics_in_batches, calculate_r2_in_batches,
    load_condition_split_csv, EarlyStopping
)
from models import TriplexPINN
from losses import My_loss


def train_one_stage(condition_name,
                    prev_state_dict=None,
                    physics_weight=2.0,
                    data_root="data"):
    """
    è®­ç»ƒæŸä¸€ä¸ªå·¥å†µï¼ˆNormal / Leak / Block / Wornï¼‰ï¼Œ
    å¦‚æœ prev_state_dict ä¸ä¸º Noneï¼Œå°±ä»å‰ä¸€é˜¶æ®µå‚æ•°ç»§ç»­è®­ç»ƒï¼ˆè¿ç§»å­¦ä¹ ï¼‰ã€‚
    """

    print(f"\n============================")
    print(f"å¼€å§‹è®­ç»ƒå·¥å†µ: {condition_name}")
    print(f"æ˜¯å¦è¿ç§»åˆå§‹åŒ–: {'æ˜¯' if prev_state_dict is not None else 'å¦'}")
    print(f"============================\n")

    # 1. åŠ è½½è¯¥å·¥å†µçš„ train/val/test
    X_train, y_train = load_condition_split_csv(
        root_dir=data_root, split="train", condition=condition_name, device=DEVICE
    )
    X_val, y_val = load_condition_split_csv(
        root_dir=data_root, split="val", condition=condition_name, device=DEVICE
    )
    X_test, y_test = load_condition_split_csv(
        root_dir=data_root, split="test", condition=condition_name, device=DEVICE
    )

    inputs_train, inputs_val, inputs_test = X_train, X_val, X_test
    targets_train, targets_val, targets_test = y_train, y_val, y_test

    # 2. æ ‡å‡†åŒ–ï¼ˆåªç”¨æœ¬å·¥å†µçš„ train å» fitï¼‰
    num_train = inputs_train.shape[0]
    _, mean_inputs_train, std_inputs_train = standardize_tensor(
        torch.reshape(inputs_train, (num_train, 1, INPUT_DIM)), mode='fit'
    )
    _, mean_targets_train, std_targets_train = standardize_tensor(
        targets_train, mode='fit'
    )

    # 3. DataLoader
    train_set = TensorDataset(inputs_train, targets_train)
    train_loader = DataLoader(
        train_set,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=0,
        drop_last=True
    )

    # 4. åˆå§‹åŒ–æ¨¡å‹
    layers = [NUM_NEURONS] * NUM_LAYERS
    model = TriplexPINN(
        seq_len=1,
        inputs_dim=INPUT_DIM,
        outputs_dim=OUTPUT_DIM,
        layers=layers,
        scaler_inputs=(mean_inputs_train, std_inputs_train),
        scaler_targets=(mean_targets_train, std_targets_train)
    ).to(DEVICE)

    # å¦‚æœæœ‰å‰ä¸€é˜¶æ®µå‚æ•°ï¼Œåˆ™è¿ç§»
    if prev_state_dict is not None:
        model.load_state_dict(prev_state_dict)

    # 5. æŸå¤±å‡½æ•° & ä¼˜åŒ–å™¨
    criterion = My_loss()
    optimizer = torch.optim.Adam(
        list(model.parameters()) + list(criterion.parameters()),
        lr=LR
    )
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 100, gamma=0.1)

    train_losses = []
    val_losses = []
    lambda_history = []  # ğŸ”¥ è®°å½•æ¯ä¸ª epoch çš„ç‰©ç†æƒé‡ Î»
    # â­â­ 6. åˆ›å»º EarlyStoppingï¼ˆæ¯ä¸ªå·¥å†µä¸€ä¸ªç‹¬ç«‹ best_xxx.pthï¼‰ â­â­
    best_path = f"tl_results/best_{condition_name}.pth"
    os.makedirs("tl_results", exist_ok=True)
    early_stopping = EarlyStopping(
        patience=30,
        delta=1e-5,
        save_path=best_path
    )
    # 6. è®­ç»ƒå¾ªç¯ï¼ˆå’Œä½ åŸæ¥çš„ train() å‡ ä¹ä¸€æ ·ï¼‰
    for epoch in range(NUM_EPOCH):
        model.train()
        epoch_train_loss = 0.0

        with torch.backends.cudnn.flags(enabled=False):
            for period, (batch_x, batch_y) in enumerate(train_loader):
                p_pred, P_t_pred = model(inputs=batch_x)

                loss = criterion(
                    targets_P=batch_y,
                    outputs_P=p_pred,
                    dpdt=P_t_pred,
                    mdot_A=batch_x[:, 2],
                    V=2 * np.exp(-4),
                    bulk_modulus_model='const',
                    air_dissolution_model='off',
                    rho_L_atm=851.6,
                    beta_L_atm=1.46696e+03,
                    beta_gain=0.2,
                    air_fraction=0.005,
                    rho_g_atm=1.225,
                    polytropic_index=1.0,
                    p_atm=0.101325,
                    p_crit=3,
                    p_min=1
                )

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                epoch_train_loss += loss.item()

        avg_train_loss = epoch_train_loss / len(train_loader)
        train_losses.append(avg_train_loss)

        # éªŒè¯
        model.eval()
        inputs_val.requires_grad_(True)
        P_pred_val, P_t_pred_val = model(inputs=inputs_val)
        val_loss = criterion(
            targets_P=targets_val,
            outputs_P=P_pred_val,
            dpdt=P_t_pred_val,
            mdot_A=inputs_val[:, 2],
            V=2 * np.exp(-4),
            bulk_modulus_model='const',
            air_dissolution_model='off',
            rho_L_atm=851.6,
            beta_L_atm=1.46696e+03,
            beta_gain=0.2,
            air_fraction=0.005,
            rho_g_atm=1.225,
            polytropic_index=1.0,
            p_atm=0.101325,
            p_crit=3,
            p_min=1
        )
        val_losses.append(val_loss.item())
        # è®°å½•å½“å‰ epoch çš„ Î»
        lambda_history.append(criterion.physics_weight.item())
        # ğŸ”¥ è°ƒç”¨æ—©åœ
        early_stopping(val_loss.item(), model)

        print(f"[{condition_name}] Epoch {epoch + 1}/{NUM_EPOCH}, "
              f"train_loss={avg_train_loss:.5f}, val_loss={val_loss.item():.5f}, "
              f"lambda={criterion.physics_weight.item():.4f}")

        if early_stopping.early_stop:
            print(f"ğŸ›‘ {condition_name} æå‰åœæ­¢åœ¨ epoch {epoch + 1}")
            break
        scheduler.step()
        #
        # print(f"[{condition_name}] Epoch {epoch+1}/{NUM_EPOCH}, "
        #       f"train_loss={avg_train_loss:.5f}, val_loss={val_loss.item():.5f}")
    # â­ åœ¨è®¡ç®—æŒ‡æ ‡å‰ï¼Œå…ˆåŠ è½½è¯¥å·¥å†µçš„æœ€ä¼˜æ¨¡å‹å‚æ•°
    if os.path.exists(best_path):
        model.load_state_dict(torch.load(best_path, map_location=DEVICE))
        print(f"âœ” ä½¿ç”¨ {condition_name} çš„æœ€ä½³æ¨¡å‹å‚æ•°è¿›è¡Œè¯„ä¼°")
    else:
        print(f"âš  æœªæ‰¾åˆ° {best_path}ï¼Œä½¿ç”¨æœ€åä¸€è½®æ¨¡å‹è¯„ä¼°")
        # ğŸ”¥ ä¿å­˜è¯¥å·¥å†µè®­ç»ƒè¿‡ç¨‹ä¸­ Î» çš„å˜åŒ–æ›²çº¿ï¼ˆcsv + pngï¼‰
        if len(lambda_history) > 0:
            lambda_df = pd.DataFrame({
                "epoch": np.arange(1, len(lambda_history) + 1),
                "lambda": lambda_history
            })
            lambda_csv_path = os.path.join("tl_results", f"lambda_{condition_name}.csv")
            lambda_df.to_csv(lambda_csv_path, index=False)
            print(f"âœ” {condition_name} çš„ Î» å˜åŒ–å·²ä¿å­˜åˆ°: {lambda_csv_path}")

            # ç”» Î» æ›²çº¿å›¾
            plt.figure(figsize=(8, 4))
            plt.plot(lambda_df["epoch"], lambda_df["lambda"], marker="o")
            plt.xlabel("Epoch")
            plt.ylabel("Lambda (physics_weight)")
            plt.title(f"Lambda evolution - {condition_name}")
            plt.grid(True)

            lambda_png_path = os.path.join("tl_results", f"lambda_{condition_name}.png")
            plt.savefig(lambda_png_path, dpi=300, bbox_inches="tight")
            plt.close()
            print(f"âœ” {condition_name} çš„ Î» æ›²çº¿å›¾å·²ä¿å­˜åˆ°: {lambda_png_path}")
    # 7. åœ¨ train/val/test ä¸Šåˆ†åˆ«ç®—æŒ‡æ ‡
    model.eval()
    P_pred_train, _ = model(inputs=inputs_train)
    P_pred_val, _ = model(inputs=inputs_val)
    P_pred_test, _ = model(inputs=inputs_test)

    rmse_train, mae_train, mape_train = calculate_metrics_in_batches(P_pred_train, targets_train)
    rmse_val, mae_val, mape_val = calculate_metrics_in_batches(P_pred_val, targets_val)
    rmse_test, mae_test, mape_test = calculate_metrics_in_batches(P_pred_test, targets_test)

    r2_train = calculate_r2_in_batches(P_pred_train, targets_train)
    r2_val   = calculate_r2_in_batches(P_pred_val, targets_val)
    r2_test  = calculate_r2_in_batches(P_pred_test, targets_test)

    metrics = {
        "train": {"RMSE": rmse_train.item(), "MAE": mae_train.item(),
                  "MAPE": mape_train.item(), "R2": r2_train.item()},
        "val":   {"RMSE": rmse_val.item(),   "MAE": mae_val.item(),
                  "MAPE": mape_val.item(),   "R2": r2_val.item()},
        "test":  {"RMSE": rmse_test.item(),  "MAE": mae_test.item(),
                  "MAPE": mape_test.item(),  "R2": r2_test.item()},
    }

    print(f"\n[{condition_name}] æŒ‡æ ‡ï¼š")
    for split in ["train", "val", "test"]:
        m = metrics[split]
        print(f"  {split}: RMSE={m['RMSE']:.4f}, MAE={m['MAE']:.4f}, "
              f"MAPE={m['MAPE']:.2f}, R2={m['R2']:.4f}")

    # è¿”å›å½“å‰æ¨¡å‹çš„å‚æ•°ï¼ˆç”¨äºè¿ç§»ï¼‰å’ŒæŒ‡æ ‡
    return model.state_dict(), metrics, (train_losses, val_losses)


def main_tl():
    # è¿ç§»é¡ºåºï¼šNormal -> Leak -> Block -> Worn
    condition_order = ["Normal", "Leak", "Block", "Worn"]
    physics_weight = 2.0  # ä½ åŸæ¥çš„ My_loss(weight)

    prev_state_dict = None
    all_stage_metrics = {}
    all_stage_losses = {}
    start_time = time.time()
    for idx, cond in enumerate(condition_order):
        # ç¬¬ä¸€ä¸ªå·¥å†µ prev_state_dict=None -> éšæœºåˆå§‹åŒ–
        # åé¢çš„å·¥å†µ prev_state_dict!=None -> è¿ç§»å­¦ä¹ 
        prev_state_dict, metrics, losses = train_one_stage(
            condition_name=cond,
            prev_state_dict=prev_state_dict,
            physics_weight=physics_weight,
            data_root="data"
        )

        all_stage_metrics[cond] = metrics
        all_stage_losses[cond] = losses  # (train_losses, val_losses)
        # Record end time and calculate total duration
    end_time = time.time()
    total_training_time = end_time - start_time
    # å¯ä»¥æŠŠ all_stage_metrics å­˜æˆä¸€ä¸ª json / csvï¼Œæ–¹ä¾¿è®ºæ–‡ç”»å›¾/åšè¡¨
    os.makedirs("tl_results", exist_ok=True)
    # Generate timestamp for unique filenames
    timestamp = time.strftime("%Y%m%d-%H%M%S")
    rows = []
    for cond, ms in all_stage_metrics.items():
        for split in ["train", "val", "test"]:
            rows.append({
                "condition": cond,
                "split": split,
                "RMSE": ms[split]["RMSE"],
                "MAE": ms[split]["MAE"],
                "MAPE": ms[split]["MAPE"],
                "R2": ms[split]["R2"],
            })
    df = pd.DataFrame(rows)
    # Include timestamp and training time in the filename
    metrics_filename = f"tl_stage_metrics_{timestamp}.csv"
    metrics_path = os.path.join("tl_results", metrics_filename)
    df.to_csv(metrics_path, index=False)

    # Save training time to a separate file
    training_info = {
        "timestamp": timestamp,
        "total_training_time_seconds": total_training_time,
        "conditions_trained": ", ".join(condition_order)
    }
    training_info_df = pd.DataFrame([training_info])
    training_info_filename = f"training_time_{timestamp}.csv"
    training_info_path = os.path.join("tl_results", training_info_filename)
    training_info_df.to_csv(training_info_path, index=False)

    print(f"\nTotal training time: {total_training_time:.2f} seconds")
    print(f"TL-PINN å„é˜¶æ®µæŒ‡æ ‡å·²ä¿å­˜åˆ° {metrics_path}")
    print(f"Training time info saved to {training_info_path}")

    # === æ–°å¢ï¼šç”¨â€œæœ€ç»ˆæ¨¡å‹â€ç»Ÿä¸€è¯„ä¼°å››ä¸ªå·¥å†µçš„ test é›†ï¼Œå¹¶å•ç‹¬ä¿å­˜ =================
    print("\n================ ä½¿ç”¨æœ€ç»ˆæ¨¡å‹è¯„ä¼°å››ä¸ªå·¥å†µçš„æµ‹è¯•é›† ================")
    final_test_metrics = evaluate_final_model_on_all_tests(
        final_state_dict=prev_state_dict,  # æœ€åä¸€ä¸ªé˜¶æ®µè¿”å›çš„ best state_dict
        condition_order=condition_order,
        data_root="data"
    )

    rows_final = []
    for cond, m in final_test_metrics.items():
        rows_final.append({
            "condition": cond,
            "split": "test_final_model",
            "RMSE": m["RMSE"],
            "MAE": m["MAE"],
            "MAPE": m["MAPE"],
            "R2": m["R2"],
        })
    df_final = pd.DataFrame(rows_final)
    final_filename = f"tl_final_model_test_metrics_{timestamp}.csv"
    final_path = os.path.join("tl_results", final_filename)
    df_final.to_csv(final_path, index=False)

    print(f"æœ€ç»ˆæ¨¡å‹åœ¨å››ä¸ªæµ‹è¯•é›†ä¸Šçš„æŒ‡æ ‡å·²ä¿å­˜åˆ° {final_path}")
    # ==================================================================
# === æ–°å¢ï¼šç”¨â€œæœ€ç»ˆæ¨¡å‹æƒé‡â€ç»Ÿä¸€è¯„ä¼°å››ä¸ªå·¥å†µçš„ test é›† ==================
def evaluate_final_model_on_all_tests(final_state_dict,
                                      condition_order,
                                      data_root="data"):
    """
    ä½¿ç”¨æœ€åé˜¶æ®µå¾—åˆ°çš„æœ€ç»ˆæ¨¡å‹å‚æ•°ï¼ˆfinal_state_dictï¼‰ï¼Œ
    å¯¹æ¯ä¸€ä¸ªå·¥å†µçš„ test é›†è¿›è¡Œç»Ÿä¸€æŒ‡æ ‡è¯„ä¼°ã€‚
    """
    layers = [NUM_NEURONS] * NUM_LAYERS
    results = {}

    for cond in condition_order:
        print(f"\n[Final model] å¼€å§‹è¯„ä¼°å·¥å†µ {cond} çš„ test é›†")

        # 1) è¯»å–è¯¥å·¥å†µçš„ train / testï¼Œç”¨ train ç®—æ ‡å‡†åŒ–ï¼ˆå’Œ train_one_stage ä¿æŒä¸€è‡´ï¼‰
        X_train, y_train = load_condition_split_csv(
            root_dir=data_root, split="train", condition=cond, device=DEVICE
        )
        X_test, y_test = load_condition_split_csv(
            root_dir=data_root, split="test", condition=cond, device=DEVICE
        )

        num_train = X_train.shape[0]
        _, mean_inputs_train, std_inputs_train = standardize_tensor(
            torch.reshape(X_train, (num_train, 1, INPUT_DIM)), mode='fit'
        )
        _, mean_targets_train, std_targets_train = standardize_tensor(
            y_train, mode='fit'
        )

        # 2) æ„å»ºæ¨¡å‹å¹¶åŠ è½½â€œæœ€ç»ˆæ¨¡å‹â€çš„æƒé‡
        model = TriplexPINN(
            seq_len=1,
            inputs_dim=INPUT_DIM,
            outputs_dim=OUTPUT_DIM,
            layers=layers,
            scaler_inputs=(mean_inputs_train, std_inputs_train),
            scaler_targets=(mean_targets_train, std_targets_train)
        ).to(DEVICE)
        model.load_state_dict(final_state_dict)
        model.eval()

        with torch.no_grad():
            P_pred_test, _ = model(inputs=X_test)

        rmse_test, mae_test, mape_test = calculate_metrics_in_batches(
            P_pred_test, y_test
        )
        r2_test = calculate_r2_in_batches(P_pred_test, y_test)

        print(f"[Final model | {cond} - test] "
              f"RMSE={rmse_test:.4f}, MAE={mae_test:.4f}, "
              f"MAPE={mape_test:.2f}, R2={r2_test:.4f}")

        results[cond] = {
            "RMSE": rmse_test.item(),
            "MAE": mae_test.item(),
            "MAPE": mape_test.item(),
            "R2": r2_test.item()
        }

    return results
if __name__ == "__main__":
    main_tl()
